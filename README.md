# Neural Network Regularization & Optimizer Comparison with the Iris Dataset

ğŸ“Œ Overview
In this project, I explored the impact of different regularization techniques and optimizers on a neural network trained on the Iris dataset. The primary goal was to understand how regularization methods affect overfitting and how different optimizers impact convergence speed.

ğŸ“ Project Highlights
âœ… Built a **baseline model** without regularization.  
âœ… Implemented models with **L1 Regularization**, **L2 Regularization**, and **Dropout**.  
âœ… Compared optimizers: **Adam, Momentum, and RMSProp**.  
âœ… Visualized training loss curves for all models.  
âœ… Analyzed convergence behavior of optimizers.

---

ğŸ“‚ Dataset
The Iris dataset** contains 150 samples of iris flowers categorized into three classes:
- Setosa
- Versicolor
- Virginica

Each sample has four features**:
- Sepal length
- Sepal width
- Petal length
- Petal width

The target labels were one-hot encoded for multi-class classification.

----


ğŸ”¹ Results & Observations
- Momentum & RMSProp converged faster than Adam
- Adam combines both methods, but the slow convergence of Momentum affected its performance.
- RMSProp performed the best in terms of speed.

---

ğŸ“Š Results
ğŸ“‰ Loss vs. Epochs for Different Regularization Techniques
![Loss vs Epochs](./loss_vs_epochs.png)

ğŸ“‰ Optimizer Convergence Analysis
![Optimizer Convergence](./optimizer_comparison.png)

---

ğŸ¯ Key Takeaways
- Regularization prevents overfitting but impacts model convergence.
- Dropout improves generalization but requires tuning.
- Adamâ€™s slower convergence is due to its dependency on Momentum and RMSProp.
- RMSProp achieved the best convergence speed.

---

ğŸ† Conclusion
This project helped me understand the practical trade-offs between regularization techniques and optimizers. Fine-tuning these aspects is crucial for building robust deep-learning models!

ğŸ”— Letâ€™s connect! If you have any insights or suggestions, feel free to open an issue or fork this repo. ğŸš€


